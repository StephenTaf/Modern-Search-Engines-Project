{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4f6ee898-7c36-44ce-870f-ca1beaf2df1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (248642413.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [119]\u001b[0;36m\u001b[0m\n\u001b[0;31m    \"Connection\": \"keep-alive\"\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import bisect #module for binary search\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "###############################################\n",
    "# only for testing\n",
    "headers = {\n",
    "    \"User-Agent\": \"DSEUniProjectCrawler/1.0 (contact: poncho.prime-3y@icloud.com)\", \n",
    "    #We don't have a webpage for our crawler\n",
    "    \"From\": \"poncho.prime-3y@icloud.com\",\n",
    "    # the different formats our crawler accepts and the preferences\n",
    "    \"Accept\": \"text/html,application/xhtml+xml q= 0.9,application/xml;q=0.8,*/*;q=0.7\",\n",
    "    # the languages our crawler accepts\n",
    "    \"Accept-Language\": \"de-DE,de;q=0.9,en;q=0.8\",\n",
    "    \"Connection\": \"keep-alive\"\n",
    "}\n",
    "\n",
    "url = \"https://www.yale.edu/robots.txt\"\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "\n",
    "# returns a dictionary of form {delay: <delay value>,explicitely allowed pages: <DictionaryOfPages> forbidden pages: <DictionaryOfPages>}}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#counts the number of lines starting with Disallow (as it happens, the yale website only has one agent specified,\n",
    "# namely the general one):\n",
    "allowdCount = response.text.count(\"Disallow:\")\n",
    "#print(f\"So many 'Allow''s are in the text {allowdCount}\")\n",
    "\n",
    "###############################################\n",
    "\n",
    "###### some global variables ########\n",
    "# for the meaning of weighted Sample Sum, weighted Sample Number, Last time-point (t_{i-1}) see the paper in the commentary of UTEMA\n",
    "''' data structure used by UTEMA '''\n",
    "# <domain> :{\"S_last\": <=weightedSampleSum>,\"N_last\": <weightedSampleNumber> \"t_last\":<lastTimePoint>, \"\"}\n",
    "responseTimes = {}\n",
    "\n",
    "####### just for testing of UTEMA:\n",
    "randomDelays = []\n",
    "\n",
    "\n",
    "\n",
    "# adds a new item to an already lexicographically ordered list\n",
    "def addItem(lst, item):\n",
    "    i = bisect.bisect_left(lst, item)\n",
    "\n",
    "    if i < len(lst)-1:\n",
    "        if item != lst[i+1]:\n",
    "            lst.insert(i+1, item)\n",
    "    else:\n",
    "        lst.insert(i+1, item)\n",
    "\n",
    "    return lst\n",
    "\n",
    "\n",
    "\n",
    "def extractTheRobotsFile(text):\n",
    "    textList = text.splitlines()\n",
    "    textList = [''.join(a.split()) for a in textList if a != '']\n",
    "    textList = [a for a in textList if not a.startswith('#')]\n",
    "    textList1 = [a.lower() for a in textList]\n",
    "    robotsDictionary = {\"delay\": 1.5, \"allowed\": [], \"forbidden\": [] , \"sitemap\": \"\" }\n",
    "    rulesStart = False\n",
    "    agentBoxStart = False\n",
    "    \n",
    "\n",
    "    for index in range(len(textList)):\n",
    "        item = textList[index]\n",
    "        item1 = textList1[index]\n",
    "\n",
    "        if not agentBoxStart:\n",
    "            agentBoxStart = item1.startswith(\"user-agent:*\") or item1.startswith(\"user-agent:dseuniprojectcrawler/1.0\")\n",
    "\n",
    "        if agentBoxStart & (rulesStart == False):\n",
    "            if index != len(textList):\n",
    "                if not textList1[index].startswith(\"user-agent\"):\n",
    "                    rulesStart = True\n",
    "\n",
    "                    \n",
    "        if agentBoxStart & rulesStart:\n",
    "            indexOfColon = item.find(\":\")\n",
    "            key = item1[0:indexOfColon]\n",
    "            if key == \"allow\":\n",
    "                addItem(robotsDictionary[\"allowed\"], item[indexOfColon+1:])\n",
    "            elif key == \"disallow\":\n",
    "                addItem(robotsDictionary[\"forbidden\"], item[indexOfColon+1:])\n",
    "            elif key == \"crawl-delay\":\n",
    "                robotsDictionary[\"delay\"] = float(item[indexOfColon+1:])\n",
    "            elif key == \"sitemap\":\n",
    "                robotsDictionary[\"sitemap\"] = item[indexOfColon+1:]\n",
    "            elif key == \"user-agent\":\n",
    "                agentBocStart = False\n",
    "                rulesStart = False\n",
    "            else:\n",
    "                raise ValueError(f\"Somehow the implemented rules are not sufficient, there is a word {key} at the beginning of the file\")\n",
    "\n",
    "    return robotsDictionary\n",
    "\n",
    "\n",
    "\n",
    "# this function returns the url where the robots.txt file is located, if it exists\n",
    "# according to RFC9309 (https://www.rfc-editor.org/rfc/rfc9309.html#name-locating-the-robots-txt-fil), this is \n",
    "# the case, if we leave the url before the 3rd \"/\" unchanged and simply add robots.txt to it\n",
    "def robotsTxtUrl(url):\n",
    "    counter = 0\n",
    "    robotsUrl = \"\"\n",
    "    for index in range(len(url)):\n",
    "        if url[index] == '/':\n",
    "            counter+=1\n",
    "        if counter == 3:\n",
    "            robotsUrl = url[0:index+1] + 'robots.txt'\n",
    "            break\n",
    "    return robotsUrl\n",
    "\n",
    "# given a series utilises UEMA (Unbiased Exponential Moving Average, from Menth et al.: \"On moving Averages, Histograms and Time- Dependent\n",
    "# Rates for Online Measurement (https://atlas.cs.uni-tuebingen.de/~menth/papers/Menth17c.pdf))\n",
    "# Note that cases t<t_0 and t <t_i < t_{i+1} are ignored for the calculation of S and N, since we only measure  (approximately) as\n",
    "# soon as we have a new data point\n",
    "def UTEMA(domain,value):\n",
    "    t = time.perf_counter()\n",
    "    beta = 1/1000000\n",
    "    if domain not in responseTimes:\n",
    "        # this will be the final weighted the average A after inclusion of the current data point\n",
    "        A = 0\n",
    "        # these are the Values for S and N in case t = t_0\n",
    "        S = value\n",
    "        N = 1 \n",
    "        # --- measures time since a certain arbitrary point,at some moment somewhen before the start of the program\n",
    "        # --- time is measured in seconds\n",
    "        responseTimes[domain] = {\"S_last\":S, \"N_last\":N, \"t_last\": t }\n",
    "\n",
    "   \n",
    "    if responseTimes[domain] != None:\n",
    "    # these are the cases t= t_i \n",
    "        S = responseTimes[domain][\"S_last\"]\n",
    "        N = responseTimes[domain][\"N_last\"]\n",
    "        t_last = responseTimes[domain][\"t_last\"]\n",
    "        expWeight = math.exp(- beta *(t - t_last))\n",
    "\n",
    "        S = expWeight * S + value\n",
    "        N = expWeight * N + 1\n",
    "\n",
    "    # updating the values in responseTime\n",
    "    responseTimes[domain][\"S_last\"] = S\n",
    "    responseTimes[domain][\"t_last\"] = t\n",
    "    responseTimes[domain][\"N_last\"] = N\n",
    "\n",
    "    # calculation of A \n",
    "    A = S / N\n",
    "\n",
    "    return A\n",
    "\n",
    "\n",
    "\n",
    "# this function here is just for plotting response- data, given a domain-name with a list of pairs (responseTime, delay-time)\n",
    "def plotResponses(responseTimeData,style):\n",
    "    time = 0\n",
    "    y = [item[1] for item in responseTimeData]\n",
    "    x = [item[0] for item in responseTimeData]\n",
    "\n",
    "    for item in x:\n",
    "        time = time + item\n",
    "        item = time\n",
    "        \n",
    "    plt.plot(x, y, style)\n",
    "    plt.xlabel('timeline of data points')\n",
    "    plt.ylabel('response Time')\n",
    "\n",
    "\n",
    "\n",
    "# this function is just for generating random Data in order to testplotResponses\n",
    "def testData(a):\n",
    "    # just part of the test if UTEMA works correctly\n",
    "    # global randomDelays\n",
    "    delayList = np.random.uniform(10**(-6),2* 10**(-6),10**6)\n",
    "    valueList = np.random.exponential(a, 10**6)\n",
    "    randomDelays = valueList\n",
    "\n",
    "    dataPointsx = []\n",
    "    dataPointsy = []\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    responses = []\n",
    "\n",
    "    for index in range(len(delayList)):\n",
    "        responses.append([0,UTEMA(\"test\", valueList[index])])\n",
    "        time.sleep(delayList[index])\n",
    "        responses[index] [0] = responseTimes[\"test\"][\"t_last\"] \n",
    "        dataPointsx.append(responseTimes[\"test\"][\"t_last\"])\n",
    "        dataPointsy.append(valueList[index])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    plotResponses(responses, '--r')\n",
    "    plt.figure()\n",
    "    plt.plot(dataPointsx, dataPointsy)\n",
    "    plt.xlabel('timeline of data points')\n",
    "    plt.ylabel('response Time')\n",
    "    \n",
    "        \n",
    "\n",
    "# this is just to test if UTEMA works correctly:\n",
    "#testData(4)\n",
    "#print(responseTimes[\"test\"][\"S_last\"] / responseTimes[\"test\"][\"N_last\"])\n",
    "#randomDelays = np.array(randomDelays)\n",
    "# print(np.mean(randomDelays))\n",
    "           \n",
    "\n",
    "    \n",
    "\n",
    "# utilises U`tEMA (Unbiased Time- Exponential Moving Average, from Menth et al.: \"On moving Averages, Histograms and Time- Dependent\n",
    "# Rates for Online Measurement (https://atlas.cs.uni-tuebingen.de/~menth/papers/Menth17c.pdf))\n",
    "def delayTime(delay):\n",
    "    ''' calculates what the crawl- delay should be, i.e., the time, until the next \n",
    "    crawler is allowed to crawl the domain '''\n",
    "    pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# this function does the following: \n",
    "# - first check if the url is english or not\n",
    "# - check if thecurl is related to tübingen (in order to determine this, it can make sense to consider the incoming link)\n",
    "# in order to do this, it might also be of importance to know if it is the subdomain of an already crawled url\n",
    "\n",
    "def singleCrawler(url, IncomingLink):\n",
    "    rules = extractTheRobotsFile(robotsTxtUrl(url))\n",
    "    \n",
    "    return []\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5da60f7-a686-46ce-885d-9c6399b842ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2089dad6-d7d1-4d0a-8c9a-e9312ef5ec92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
