{\rtf1\ansi\ansicpg1252\cocoartf2822
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fswiss\fcharset0 Helvetica-Oblique;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww28300\viewh17700\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 1. How to detect if a Website is English and associated with T\'fcbingen?\
english:\
- use 20 common english words (common in text), and look if those are closeby\
- if this happens multiple times on the page we are good\
\
associated with T\'fcbingen:\
just search if words like T\'fcbingen/ T\'fcbinger tuebingen etc. appear on the page\
\
\
ONLY FOR CLEANING OUT GARBAGE- URLS\
- additionally: Have a list of learned words (names) which are associated with t\'fcbingen ONLY\
(extend the url list by these words)\
\
\
\
\
\
\
\
2. How to deal with errors in the url- calling process?\
\
\
\
\
\
\
3. How can the crawler be implemented in order for picking up and restarting the crawling process to work?\
- need the database of the urls to be sorted in some way (just hashes and then alphabetical sorting maybe?)\
- How to store this database of urls?\
- \
\
\
4. How to avoid being blocked?\
- respect the robots.txt file \
\
\
\
5. As soon as the list of urls is done:\
How to structure the data for efficient retrieval ?\
- find sensible categories (machine learning?)\
\
\
6. How to deal with sub- domains? \
\
\
7. How to deal with websites with too many links on them (e.g. uni library)?\
\
\
\
A BASIS MODEL:\

\f1\i The crawler:
\f0\i0 \
- url- finding on website\
\
fiding the robot.txt- file \
\
\
- needs to search level - wise -> priority queue (or just queue?), remember the current website (in order to continue the search)\
\
Storaage: In large database via hash- functions, reorganisation of structures if need be (if file becomes too big, split into new files (different hash- function? Alphabetical order?)\
\
\
\
\
\
allright, so obviously a first attempt on a crawler could be just moving through layers}