import duckdb
from sentence_transformers import SentenceTransformer
import logging
import config as cfg
from duckdb.typing import VARCHAR
import torch
from typing import List
import torch
class TextEmbedder:
    def __init__(self, db_path: str, embedding_model: str = cfg.EMBEDDING_MODEL, read_only: bool = True):
        if torch.cuda.is_available():
            device = "cuda"
        elif torch.backends.mps.is_available():
            device = "mps"
        else:
            device = "cpu"
        self.device = device
        logging.info(f"Using device: {self.device}")
        
        self.db_path = db_path
        self.vdb = duckdb.connect(db_path, read_only=read_only)    
        self.embedding_model = SentenceTransformer(embedding_model, device=device)
        self.read_only = read_only
        self._setup_database()
        self._setup_vector_index()
         
    def _setup_database(self):
        
        """Create optimized table structure"""
        if not self.read_only:
            self.vdb.execute("""
                CREATE TABLE IF NOT EXISTS chunks_optimized(
                chunk_id BIGINT PRIMARY KEY,
                doc_id      BIGINT,
                chunk_text TEXT,
                -- FOREIGN KEY (doc_id) REFERENCES urlsDB(id)
                );
            """)
            
            # Create indexes
            self.vdb.execute("CREATE INDEX IF NOT EXISTS idx_chunks_opt_doc_id ON chunks_optimized(doc_id);")
        
        # Separate table for embeddings 
        
    def _setup_vector_index(self):
        if not self.read_only:
            self.vdb.execute(f"""
                CREATE TABLE IF NOT EXISTS embeddings(
                chunk_id BIGINT PRIMARY KEY,
                embedding FLOAT[{cfg.EMBEDDING_DIMENSION}],
                );
            """)
 
        def _embed(chunk: str):
            """
            Embed a single chunk using the embedding model.
            """
            return self.embedding_model.encode(chunk, normalize_embeddings=True, show_progress_bar=False, device=self.device).tolist()  # Convert to list for DuckDB compatibility
            #normalize the embedding to unit length
            
        self.vdb.create_function("embed", _embed, [VARCHAR], f'FLOAT[{cfg.EMBEDDING_DIMENSION}]') 
    
  
    
    def create_sliding_windows(self, text: str, window_size: int, step_size: int) -> List[List[int]]:
        tokens = self.embedding_model.tokenizer.encode(text, add_special_tokens=False)
        if len(tokens) <= window_size:
            return [tokens]
        
        windows = []
        # Generate all possible windows using step_size
        for i in range(0, len(tokens) - window_size + 1, step_size):
            window = tokens[i:i + window_size]
            windows.append(window)
        
        # Calculate start index for the last full-size window
        last_window_start = len(tokens) - window_size
        
        # Add last full window if:
        # 1. It starts at a valid position (non-negative)
        # 2. It wasn't already generated by the loop
        #    (i.e., its start index isn't a multiple of step_size)
        if last_window_start >= 0 and last_window_start % step_size != 0:
            last_window = tokens[last_window_start:last_window_start + window_size]
            windows.append(last_window)
        
        return windows
    
    def prepare_window_texts(self, windows: List[List[int]]) -> List[str]:
        """Batch decode token windows"""
        window_texts = []
        for window in windows:
            # Decode window tokens back to text (without special tokens)
            window_text = self.embedding_model.tokenizer.decode(window, skip_special_tokens=True)
            window_texts.append(window_text)
        return window_texts
    
 